# Training Deep Neural Networks
Explore the vanishing and exploding gradients problems, tackle complex tasks when you have little labeled data, various optimizers to speed up training models and a few popular regularization techniques.

## Contents
* The Vanishing / Exploding Gradients Problems
  * Weight Initialization
  * Activation Functions
  * Batch Normalization
  * Gradient Clipping
* Reusing Pretraining Layers
  * Transfer learning with Keras
* Faster Optimizers
  * Momentum optimization
  * AdaGrad
  * RMSProp
  * Adam and Nadam Optimization
  * Learning Rate Scheduling
* Avoiding overfitting through Regularization
  * l1 and l2 regularization
  * Dropout
  * Monte Carlo (MC) Dropout
  * Max-Norm Regularization


## Sources
1. [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
2. [ageron/handson-ml2](https://github.com/ageron/handson-ml2)
